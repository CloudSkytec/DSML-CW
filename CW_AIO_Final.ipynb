{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff7340ab4d1fafba",
   "metadata": {},
   "source": [
    "# Dow Jones Index Trading Signal Prediction\n",
    "> dataset: https://archive.ics.uci.edu/dataset/312/dow+jones+index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296effe5ad866fb",
   "metadata": {},
   "source": [
    "### Data Loading & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360887536fb3a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T18:41:46.567316Z",
     "start_time": "2025-05-01T18:41:44.596116Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import warnings\n",
    "\n",
    "# remove warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# fetch dataset\n",
    "dow_jones_index = fetch_ucirepo(id=312)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "# print(dow_jones_index.data.features)\n",
    "# print(\"\\n----\\n\")\n",
    "# print(dow_jones_index.data.targets)\n",
    "# print(dow_jones_index.variables)\n",
    "\n",
    "df = pd.concat([dow_jones_index.data.features, dow_jones_index.data.targets], axis=1)\n",
    "df.info()\n",
    "df[\"stock\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global drawing parameters\n",
    "plt.rcParams.update({\"font.size\": 14, \"lines.linewidth\": 2, \"lines.markersize\": 8})\n",
    "\n",
    "cols_to_convert = [\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"percent_change_price\",\n",
    "    \"percent_change_volume_over_last_wk\",\n",
    "    \"previous_weeks_volume\",\n",
    "    \"next_weeks_open\",\n",
    "    \"next_weeks_close\",\n",
    "    \"days_to_next_dividend\",\n",
    "    \"percent_return_next_dividend\",\n",
    "]\n",
    "\n",
    "# remove '$' and ',' from each column and convert them to numerical values\n",
    "for col in cols_to_convert:\n",
    "    dow_jones_index.data.features[col] = (\n",
    "        dow_jones_index.data.features[col]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"[\\$,]\", \"\", regex=True)\n",
    "    )\n",
    "    dow_jones_index.data.features[col] = pd.to_numeric(\n",
    "        dow_jones_index.data.features[col], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "# date conversion\n",
    "if \"date\" in dow_jones_index.data.features.columns:\n",
    "    dow_jones_index.data.features[\"date\"] = pd.to_datetime(\n",
    "        dow_jones_index.data.features[\"date\"], errors=\"coerce\"\n",
    "    )\n",
    "else:\n",
    "    dow_jones_index.data.features[\"date\"] = pd.date_range(\n",
    "        start=\"2000-01-01\", periods=len(dow_jones_index.data.features), freq=\"W\"\n",
    "    )\n",
    "\n",
    "# sort by date\n",
    "dow_jones_index.data.features.sort_values(by=\"date\", inplace=True)\n",
    "\n",
    "# get all stock codes\n",
    "unique_stocks = sorted(dow_jones_index.data.features[\"stock\"].unique())\n",
    "\n",
    "# generate color mapping based on the number of stocks\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_stocks)))\n",
    "color_dict = dict(zip(unique_stocks, colors))\n",
    "\n",
    "start_limit = pd.to_datetime(\"2011-01-01\")\n",
    "end_limit = pd.to_datetime(\"2011-07-02\")\n",
    "unique_dates = np.array(sorted(dow_jones_index.data.features[\"date\"].unique()))\n",
    "ticks = unique_dates[(unique_dates >= start_limit) & (unique_dates <= end_limit)]\n",
    "ticks = pd.to_datetime(ticks).to_pydatetime().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ff155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(22, 20))\n",
    "\n",
    "for stock in unique_stocks:\n",
    "    stock_data = dow_jones_index.data.features[\n",
    "        dow_jones_index.data.features[\"stock\"] == stock\n",
    "    ]\n",
    "\n",
    "    ax1.plot(\n",
    "        stock_data[\"date\"],\n",
    "        stock_data[\"open\"],\n",
    "        marker=\"o\",\n",
    "        label=stock,\n",
    "        color=color_dict[stock],\n",
    "    )\n",
    "\n",
    "    ax2.plot(\n",
    "        stock_data[\"date\"],\n",
    "        stock_data[\"close\"],\n",
    "        marker=\"o\",\n",
    "        label=stock,\n",
    "        color=color_dict[stock],\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Date\", fontsize=16)\n",
    "ax1.set_ylabel(\"Open\", fontsize=16)\n",
    "ax1.set_title(\"Open Price of Stock\", fontsize=18)\n",
    "ax1.set_xlim(start_limit, end_limit)\n",
    "ax1.set_xticks(ticks)\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "ax2.set_xlabel(\"Date\", fontsize=16)\n",
    "ax2.set_ylabel(\"Close\", fontsize=16)\n",
    "ax2.set_title(\"Close Price of Stock\", fontsize=18)\n",
    "ax2.set_xlim(start_limit, end_limit)\n",
    "ax2.set_xticks(ticks)\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# add public legend\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "max_columns = 10\n",
    "ncol = min(max_columns, len(unique_stocks))\n",
    "\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.01),\n",
    "    ncol=ncol,\n",
    "    fontsize=15,\n",
    "    title=\"Stock\",\n",
    "    title_fontsize=20,\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.98, top=0.93, bottom=0.07, hspace=0.4)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(25, 14))\n",
    "\n",
    "start_limit = pd.to_datetime(\"2011-01-01\")\n",
    "end_limit = pd.to_datetime(\"2011-07-02\")\n",
    "unique_dates = np.array(sorted(dow_jones_index.data.features[\"date\"].unique()))\n",
    "ticks = unique_dates[(unique_dates >= start_limit) & (unique_dates <= end_limit)]\n",
    "ticks = pd.to_datetime(ticks).to_pydatetime().tolist()\n",
    "\n",
    "for stock in unique_stocks:\n",
    "    stock_data = dow_jones_index.data.features[\n",
    "        dow_jones_index.data.features[\"stock\"] == stock\n",
    "    ]\n",
    "    ax1.plot(\n",
    "        stock_data[\"date\"],\n",
    "        stock_data[\"volume\"],\n",
    "        marker=\"o\",\n",
    "        label=stock,\n",
    "        color=color_dict[stock],\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Date\", fontsize=16)\n",
    "ax1.set_ylabel(\"Volume\", fontsize=16)\n",
    "ax1.set_title(\"Volume Trend by Stock\", fontsize=18)\n",
    "ax1.set_xlim(start_limit, end_limit)\n",
    "ax1.set_xticks(ticks)\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "max_columns = 10\n",
    "ncol = min(max_columns, len(unique_stocks))\n",
    "ax1.legend(\n",
    "    title=\"Stock\",\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, -0.08),\n",
    "    ncol=ncol,\n",
    "    fontsize=15,\n",
    "    title_fontsize=20,\n",
    ")\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.98, top=0.93, bottom=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ec1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "dow_jones_index.data.features[\"high_low_diff\"] = (\n",
    "    dow_jones_index.data.features[\"high\"] - dow_jones_index.data.features[\"low\"]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 14))\n",
    "\n",
    "sns.boxplot(\n",
    "    x=\"stock\",\n",
    "    y=\"high_low_diff\",\n",
    "    hue=\"stock\",\n",
    "    data=dow_jones_index.data.features,\n",
    "    palette=color_dict,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.set_title(\"High-Low Price Difference Distribution\", fontsize=18)\n",
    "ax.set_xlabel(\"Stock\", fontsize=16)\n",
    "ax.set_ylabel(\"High-Low Difference\", fontsize=16)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.98, top=0.93, bottom=0.08)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = dow_jones_index.data.features.drop(columns=[\"quarter\"])\n",
    "X1 = dow_jones_index.data.features.drop(columns=[\"high_low_diff\"])\n",
    "\n",
    "numeric_cols = X1.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = X1[numeric_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 14))\n",
    "\n",
    "heatmap = sns.heatmap(\n",
    "    corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={\"shrink\": 0.8}, ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Correlation Heatmap of Numeric Features\", fontsize=18)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplots_adjust(left=0.08, right=0.98, top=0.93, bottom=0.08)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbe521",
   "metadata": {},
   "source": [
    "## <font color=Blue>**Regression Task**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49084e1c4fc3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T18:41:49.101168Z",
     "start_time": "2025-05-01T18:41:49.094991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# Remove irrelevant char\n",
    "for c in ['open', 'high', 'low', 'close', 'volume']:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "# Missing value\n",
    "df['percent_change_volume_over_last_wk'] = df['percent_change_volume_over_last_wk'].fillna(0)\n",
    "df['previous_weeks_volume'] = df['previous_weeks_volume'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3d1a4",
   "metadata": {},
   "source": [
    "### Feature Enginnerring\n",
    "- Constructing technical indicators that often use in analysing stock market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1176350499656f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T18:41:51.793978Z",
     "start_time": "2025-05-01T18:41:51.577882Z"
    }
   },
   "outputs": [],
   "source": [
    "import talib\n",
    "\n",
    "\n",
    "def dividend_category(days):\n",
    "    if days <= 7:\n",
    "        return -1\n",
    "    elif days <= 30:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def generate_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    def construct_and_fill(group):\n",
    "        group = group.copy()\n",
    "\n",
    "        # ===== Construct technical indicators =====\n",
    "        group[\"ema_3\"] = talib.EMA(group[\"close\"], timeperiod=3)\n",
    "        group[\"ema_5\"] = talib.EMA(group[\"close\"], timeperiod=5)\n",
    "        group[\"ema_10\"] = talib.EMA(group[\"close\"], timeperiod=10)\n",
    "        group[\"ema_diff_5_10\"] = group[\"ema_5\"] - group[\"ema_10\"]\n",
    "        group[\"ema_diff\"] = group[\"ema_3\"] - group[\"ema_5\"]\n",
    "        group[\"rsi_7\"] = talib.RSI(group[\"close\"], timeperiod=7)\n",
    "        group[\"rsi_14\"] = talib.RSI(group[\"close\"], timeperiod=14)\n",
    "        group[\"atr_7\"] = talib.ATR(\n",
    "            group[\"high\"], group[\"low\"], group[\"close\"], timeperiod=7\n",
    "        )\n",
    "        macd, macdsignal, macdhist = talib.MACD(\n",
    "            group[\"close\"], fastperiod=6, slowperiod=13, signalperiod=5\n",
    "        )\n",
    "        group[\"macd\"] = macd\n",
    "        group[\"macd_signal\"] = macdsignal\n",
    "        group[\"macd_hist\"] = macdhist\n",
    "        group[\"momentum_3\"] = group[\"close\"].pct_change(periods=3)\n",
    "        group[\"momentum_5\"] = group[\"close\"].pct_change(periods=5)\n",
    "        group[\"momentum_diff\"] = group[\"momentum_3\"] - group[\"momentum_5\"]\n",
    "        group[\"roc_4\"] = talib.ROC(group[\"close\"], timeperiod=4)\n",
    "        group[\"volatility_5\"] = group[\"percent_change_price\"].rolling(5).std()\n",
    "        group[\"volatility_3\"] = group[\"percent_change_price\"].rolling(3).std()\n",
    "        group[\"volatility_diff\"] = group[\"volatility_3\"] - group[\"volatility_5\"]\n",
    "        group[\"vpt\"] = (group[\"percent_change_price\"] * group[\"volume\"]).cumsum()\n",
    "        group[\"price_volume_ratio\"] = group[\"percent_change_price\"] / (\n",
    "            group[\"percent_change_volume_over_last_wk\"] + 1e-6\n",
    "        )\n",
    "        group[\"price_volume_ratio\"] = group[\"price_volume_ratio\"].clip(-10, 10)\n",
    "        group[\"is_month_end_week\"] = (\n",
    "            group[\"date\"].dt.month != group[\"date\"].shift(-1).dt.month\n",
    "        ).astype(int)\n",
    "        group[\"dividend_timing\"] = group[\"days_to_next_dividend\"].apply(\n",
    "            dividend_category\n",
    "        )\n",
    "        group[\"return_t-3\"] = group[\"percent_change_price\"].shift(3)\n",
    "        group[\"return_t-5\"] = group[\"percent_change_price\"].shift(5)\n",
    "\n",
    "        # ===== Fill missing values =====\n",
    "        indicator_cols = [\n",
    "            \"ema_3\",\n",
    "            \"ema_5\",\n",
    "            \"ema_diff\",\n",
    "            \"ema_10\",\n",
    "            \"ema_diff_5_10\",\n",
    "            \"rsi_7\",\n",
    "            \"rsi_14\",\n",
    "            \"atr_7\",\n",
    "            \"momentum_diff\",\n",
    "            \"volatility_3\",\n",
    "            \"macd\",\n",
    "            \"macd_signal\",\n",
    "            \"macd_hist\",\n",
    "            \"volatility_diff\",\n",
    "            \"momentum_3\",\n",
    "            \"momentum_5\",\n",
    "            \"roc_4\",\n",
    "            \"volatility_5\",\n",
    "            \"vpt\",\n",
    "            \"price_volume_ratio\",\n",
    "            \"return_t-3\",\n",
    "            \"return_t-5\",\n",
    "        ]\n",
    "\n",
    "        # Remove Feature\n",
    "        del group[\"previous_weeks_volume\"]\n",
    "        del group[\"quarter\"]\n",
    "        del group[\"days_to_next_dividend\"]\n",
    "\n",
    "        # Step 1: Forward fill only from first valid index\n",
    "        for col in indicator_cols:\n",
    "            first_valid = group[col].first_valid_index()\n",
    "            if first_valid:\n",
    "                group.loc[first_valid:, col] = group.loc[first_valid:, col].ffill()\n",
    "\n",
    "        # Step 2: Use expanding mean (no leakage)\n",
    "        for col in indicator_cols:\n",
    "            group[col] = group[col].fillna(group[col].expanding().mean())\n",
    "\n",
    "        # Step 3: Optional 0 fill only for specific types like momentum\n",
    "        for col in [\"momentum_3\", \"momentum_5\"]:\n",
    "            group[f\"{col}_missing\"] = group[col].isna().astype(int)\n",
    "            group[col] = group[col].fillna(0)\n",
    "\n",
    "        # Step 4: Final hard fill (if needed)\n",
    "        group[indicator_cols] = group[indicator_cols].fillna(0)\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply by stock\n",
    "    df = df.groupby(\"stock\").apply(construct_and_fill).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# df is original dataset which contains every data and attributes\n",
    "df_features = generate_features(df)\n",
    "df_features.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62fe931",
   "metadata": {},
   "source": [
    "### Analyse `percent_change_price` distribution to decide the threshold\n",
    "- It's a foundation for data analayst or manager to decide how to set threshold\n",
    "- Here we decide to use 25% and 75% as our threshold, hence constructing bondary of BUY,HOLD and SELL signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045184c13cfdd8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T18:42:13.190728Z",
     "start_time": "2025-05-01T18:42:12.844858Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features[\"percent_change_price\"].describe()\n",
    "plt.hist(df_features[\"percent_change_next_weeks_price\"], bins=100)\n",
    "plt.title(\"Distribution of Next Week Price Change (%)\")\n",
    "plt.xlabel(\"Percent Change\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "df[\"percent_change_next_weeks_price\"].plot(kind=\"kde\")\n",
    "plt.title(\"KDE of Next Week Price Change\")\n",
    "plt.xlabel(\"Percent Change\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "threshold_up = 1.28\n",
    "threshold_down = -1.65\n",
    "\n",
    "plt.hist(df[\"percent_change_next_weeks_price\"], bins=100, alpha=0.7)\n",
    "plt.axvline(threshold_up, color=\"red\", linestyle=\"--\", label=\"Buy Threshold\")\n",
    "plt.axvline(threshold_down, color=\"green\", linestyle=\"--\", label=\"Sell Threshold\")\n",
    "plt.legend()\n",
    "plt.title(\"Thresholds on Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620929e0ae83870",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955cef42e32bbd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T18:43:01.532610Z",
     "start_time": "2025-05-01T18:43:01.450764Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class BaseSignalModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        feature_cols=None,\n",
    "        threshold=2,\n",
    "        name=None,\n",
    "        risk_free_rate=0.02,\n",
    "        train_ratio=0.8,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.name = name or model.__class__.__name__\n",
    "        self.scaler = StandardScaler()\n",
    "        self.threshold = threshold\n",
    "        self.feature_cols = feature_cols\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        self.metrics = {}\n",
    "        self.support_cv = True\n",
    "        self.train_ratio = train_ratio\n",
    "\n",
    "    def fit(self, df, cv_splits=None):\n",
    "        df = df.copy()\n",
    "\n",
    "        if self.feature_cols is None:\n",
    "            self.feature_cols = [\n",
    "                col\n",
    "                for col in df.columns\n",
    "                if col\n",
    "                not in [\n",
    "                    \"stock\",\n",
    "                    \"date\",\n",
    "                    \"percent_change_next_weeks_price\",\n",
    "                    \"next_weeks_open\",\n",
    "                    \"next_weeks_close\",\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "        df = df.sort_values(by=[\"stock\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "        train_list = []\n",
    "        test_list = []\n",
    "\n",
    "        for stock in df[\"stock\"].unique():\n",
    "            stock_data = df[df[\"stock\"] == stock]\n",
    "            n_train = int(len(stock_data) * self.train_ratio)\n",
    "            train_data = stock_data.iloc[:n_train]\n",
    "            test_data = stock_data.iloc[n_train:]\n",
    "            train_list.append(train_data)\n",
    "            test_list.append(test_data)\n",
    "\n",
    "        train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "        test_df = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "        X_train = train_df[self.feature_cols]\n",
    "        y_train = train_df[\"percent_change_next_weeks_price\"]\n",
    "\n",
    "        X_test = test_df[self.feature_cols]\n",
    "        y_test = test_df[\"percent_change_next_weeks_price\"]\n",
    "\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "\n",
    "        # Cross validation\n",
    "        if cv_splits is not None and cv_splits >= 2:\n",
    "            tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "\n",
    "            cv_rmse = []\n",
    "            cv_mae = []\n",
    "            cv_r2 = []\n",
    "            cv_direction_acc = []\n",
    "\n",
    "            for train_index, val_index in tscv.split(X_train_scaled):\n",
    "                X_tr, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "                y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                model_clone = self._clone_model()\n",
    "                model_clone.fit(X_tr, y_tr)\n",
    "                y_val_pred = model_clone.predict(X_val)\n",
    "\n",
    "                cv_rmse.append(root_mean_squared_error(y_val, y_val_pred))\n",
    "                cv_mae.append(mean_absolute_error(y_val, y_val_pred))\n",
    "                cv_r2.append(r2_score(y_val, y_val_pred))\n",
    "\n",
    "                pred_direction = np.where(\n",
    "                    y_val_pred > self.threshold,\n",
    "                    1,\n",
    "                    np.where(y_val_pred < -self.threshold, -1, 0),\n",
    "                )\n",
    "                true_direction = np.where(\n",
    "                    y_val > self.threshold, 1, np.where(y_val < -self.threshold, -1, 0)\n",
    "                )\n",
    "                self.metrics[\"Direction_Accuracy\"] = np.mean(\n",
    "                    pred_direction == true_direction\n",
    "                )\n",
    "                cv_direction_acc.append(np.mean(pred_direction == true_direction))\n",
    "\n",
    "            self.metrics[\"CV_RMSE\"] = np.mean(cv_rmse)\n",
    "            self.metrics[\"CV_MAE\"] = np.mean(cv_mae)\n",
    "            self.metrics[\"CV_R2\"] = np.mean(cv_r2)\n",
    "            self.metrics[\"CV_Direction_Accuracy\"] = np.mean(cv_direction_acc)\n",
    "\n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "\n",
    "        self.metrics[\"RMSE\"] = root_mean_squared_error(y_test, y_pred)\n",
    "        self.metrics[\"MAE\"] = mean_absolute_error(y_test, y_pred)\n",
    "        self.metrics[\"R2\"] = r2_score(y_test, y_pred)\n",
    "\n",
    "        self.metrics[\"Direction_Accuracy\"] = self.direction_accuracy(\n",
    "            y_pred, y_test, self.threshold\n",
    "        )\n",
    "\n",
    "    def direction_accuracy(self, y_pred, y_test, threshold):\n",
    "        pred_direction = np.where(\n",
    "            y_pred > threshold, 1, np.where(y_pred < -threshold, -1, 0)\n",
    "        )\n",
    "        true_direction = np.where(\n",
    "            y_test > threshold, 1, np.where(y_test < -threshold, -1, 0)\n",
    "        )\n",
    "        return np.mean(pred_direction == true_direction)\n",
    "\n",
    "    def _clone_model(self):\n",
    "        from sklearn.base import clone\n",
    "\n",
    "        return clone(self.model)\n",
    "\n",
    "    def predict(self, df):\n",
    "        df = df.copy()\n",
    "        X_scaled = self.scaler.transform(df[self.feature_cols])\n",
    "        df[\"predicted_return\"] = self.model.predict(X_scaled)\n",
    "        return df\n",
    "\n",
    "    def generate_signal(self, df, threshold=None):\n",
    "        threshold = threshold if threshold is not None else self.threshold\n",
    "        df = df.copy()\n",
    "        df[\"signal\"] = 0\n",
    "        df.loc[df[\"predicted_return\"] > threshold, \"signal\"] = 1\n",
    "        df.loc[df[\"predicted_return\"] < -threshold, \"signal\"] = -1\n",
    "        return df\n",
    "\n",
    "    def backtest(self, df):\n",
    "        df = df.copy()\n",
    "        df[\"percent_change_next_weeks_price\"] = (\n",
    "            df[\"percent_change_next_weeks_price\"] / 100\n",
    "        )\n",
    "        df[\"strategy_return\"] = df[\"signal\"] * df[\"percent_change_next_weeks_price\"]\n",
    "        df[\"cumulative_strategy_return\"] = (1 + df[\"strategy_return\"]).cumprod()\n",
    "        df[\"cumulative_market_return\"] = (\n",
    "            1 + df[\"percent_change_next_weeks_price\"]\n",
    "        ).cumprod()\n",
    "\n",
    "        returns = df[\"strategy_return\"]\n",
    "        periods_per_year = 52  # weekly\n",
    "\n",
    "        cagr = (df[\"cumulative_strategy_return\"].iloc[-1]) ** (\n",
    "            1 / (len(df) / periods_per_year)\n",
    "        ) - 1\n",
    "        max_drawdown = self._calculate_max_drawdown(df[\"cumulative_strategy_return\"])\n",
    "        sharpe = (returns.mean() * periods_per_year - self.risk_free_rate) / (\n",
    "            returns.std() * np.sqrt(periods_per_year)\n",
    "        )\n",
    "\n",
    "        self.metrics[\"CAGR\"] = cagr\n",
    "        self.metrics[\"Max_Drawdown\"] = max_drawdown\n",
    "        self.metrics[\"Sharpe_Ratio\"] = sharpe\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_max_drawdown(cumulative_returns):\n",
    "        peak = cumulative_returns.expanding(min_periods=1).max()\n",
    "        drawdown = (cumulative_returns - peak) / peak\n",
    "        max_drawdown = drawdown.min()\n",
    "        return max_drawdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8775c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel:\n",
    "    def __init__(self, mode=\"naive\", name=None, threshold=2):\n",
    "        assert mode in [\"naive\", \"zero\"], \"the model either be'naive' or 'zero'\"\n",
    "        self.mode = mode\n",
    "        self.name = name or f\"Baseline-{mode.capitalize()}\"\n",
    "        self.metrics = {}\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, df):\n",
    "        df = df.copy()\n",
    "        self.df_train = df\n",
    "\n",
    "    def predict(self, df):\n",
    "        df = df.copy()\n",
    "        # Use current week data to predict next week, which means gussing\n",
    "        if self.mode == \"naive\":\n",
    "            df[\"predicted_return\"] = df[\"percent_change_price\"]\n",
    "        # Always predict 0\n",
    "        elif self.mode == \"zero\":\n",
    "            df[\"predicted_return\"] = 0.0\n",
    "        return df\n",
    "\n",
    "    def generate_signal(self, df):\n",
    "        df = df.copy()\n",
    "        df[\"signal\"] = 0\n",
    "        df.loc[df[\"predicted_return\"] > self.threshold, \"signal\"] = 1\n",
    "        df.loc[df[\"predicted_return\"] < -self.threshold, \"signal\"] = -1\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, df):\n",
    "        y_true = df[\"percent_change_next_weeks_price\"]\n",
    "        y_pred = df[\"predicted_return\"]\n",
    "\n",
    "        self.metrics[\"RMSE\"] = root_mean_squared_error(y_true, y_pred)\n",
    "        self.metrics[\"MAE\"] = mean_absolute_error(y_true, y_pred)\n",
    "        self.metrics[\"R2\"] = r2_score(y_true, y_pred)\n",
    "        # Direction Accuracy Test(signal:Buy,Sell,Hold)\n",
    "        self.metrics[\"Direction_Accuracy\"] = self.direction_accuracy(\n",
    "            y_pred, y_true, self.threshold\n",
    "        )\n",
    "\n",
    "    def direction_accuracy(self, y_pred, y_test, threshold):\n",
    "        pred_direction = np.where(\n",
    "            y_pred > threshold, 1, np.where(y_pred < -threshold, -1, 0)\n",
    "        )\n",
    "        true_direction = np.where(\n",
    "            y_test > threshold, 1, np.where(y_test < -threshold, -1, 0)\n",
    "        )\n",
    "        return np.mean(pred_direction == true_direction)\n",
    "\n",
    "    def backtest(self, df):\n",
    "        df = df.copy()\n",
    "        df[\"percent_change_next_weeks_price\"] = (\n",
    "            df[\"percent_change_next_weeks_price\"] / 100\n",
    "        )\n",
    "        df[\"strategy_return\"] = df[\"signal\"] * df[\"percent_change_next_weeks_price\"]\n",
    "        df[\"cumulative_strategy_return\"] = (1 + df[\"strategy_return\"]).cumprod()\n",
    "        df[\"cumulative_market_return\"] = (\n",
    "            1 + df[\"percent_change_next_weeks_price\"]\n",
    "        ).cumprod()\n",
    "\n",
    "        returns = df[\"strategy_return\"]\n",
    "        periods_per_year = 52\n",
    "\n",
    "        cagr = (df[\"cumulative_strategy_return\"].iloc[-1]) ** (\n",
    "            1 / (len(df) / periods_per_year)\n",
    "        ) - 1\n",
    "        max_drawdown = self._calculate_max_drawdown(df[\"cumulative_strategy_return\"])\n",
    "        sharpe = (returns.mean() * periods_per_year - 0.02) / (\n",
    "            returns.std() * np.sqrt(periods_per_year)\n",
    "        ) + 1e-6\n",
    "\n",
    "        self.metrics[\"CAGR\"] = cagr\n",
    "        self.metrics[\"Max_Drawdown\"] = max_drawdown\n",
    "        self.metrics[\"Sharpe_Ratio\"] = sharpe\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_max_drawdown(cumulative_returns):\n",
    "        peak = cumulative_returns.expanding(min_periods=1).max()\n",
    "        drawdown = (cumulative_returns - peak) / peak\n",
    "        max_drawdown = drawdown.min()\n",
    "        return max_drawdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner:\n",
    "    def __init__(self, models, cv_splits=None):\n",
    "        \"\"\"\n",
    "        models: dict, e.g., {\"naive\": BaselineModel(), \"my_model\": BaseSignalModel(model)}\n",
    "        cv_splits: int or None, number of splits for TimeSeries CV\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.results = {}\n",
    "        self.cv_splits = cv_splits\n",
    "\n",
    "    def run(self, df):\n",
    "        \"\"\"\n",
    "        Train, test, generate signal, evaluate all models\n",
    "        \"\"\"\n",
    "        for name, model in self.models.items():\n",
    "            # model.fit(df, cv_splits=self.cv_splits)\n",
    "            if getattr(model, \"support_cv\", False):\n",
    "                model.fit(df, cv_splits=self.cv_splits)\n",
    "            else:\n",
    "                model.fit(df)\n",
    "\n",
    "            df_pred = model.predict(df)\n",
    "            df_pred = model.generate_signal(df_pred)\n",
    "\n",
    "            if hasattr(model, \"evaluate\"):\n",
    "                model.evaluate(df_pred)\n",
    "            if hasattr(model, \"backtest\"):\n",
    "                model.backtest(df_pred)\n",
    "\n",
    "            self.results[name] = model.metrics\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Compare all models\n",
    "        \"\"\"\n",
    "        summary_df = pd.DataFrame(self.results).T\n",
    "        summary_df = summary_df.sort_values(by=\"Sharpe_Ratio\", ascending=False)\n",
    "        return summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85f698",
   "metadata": {},
   "source": [
    "### Run and Test\n",
    "- **Model Chosen:** Random Forest Regressor,XGB Regressor, SVR\n",
    "- **Baseline Model**: Naive Model, Zero Model\n",
    "- **cv split**: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3698fa62e1e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T18:45:54.095037Z",
     "start_time": "2025-05-01T18:45:53.055640Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# 1. Feature Engineering\n",
    "df_feat = generate_features(df)\n",
    "\n",
    "# Baseline Models\n",
    "naive_baseline = BaselineModel(mode=\"naive\", threshold=1.2)\n",
    "zero_baseline = BaselineModel(mode=\"zero\", threshold=1.2)\n",
    "# Models\n",
    "rf_model = BaseSignalModel(\n",
    "    RandomForestRegressor(n_estimators=60, max_depth=4, min_samples_leaf=3),\n",
    "    threshold=1.2,\n",
    "    train_ratio=0.7,\n",
    ")\n",
    "# ridge_model = BaseSignalModel(Ridge(alpha=60), threshold=1.2,train_ratio=0.55)\n",
    "xgb_model = BaseSignalModel(\n",
    "    XGBRegressor(n_estimators=30, max_depth=3, eta=0.05, alpha=10),\n",
    "    threshold=1.2,\n",
    "    train_ratio=0.7,\n",
    ")\n",
    "svr_model = BaseSignalModel(\n",
    "    SVR(kernel=\"rbf\", degree=4, epsilon=0.10, C=2), threshold=1.2, train_ratio=0.7\n",
    ")\n",
    "\n",
    "# 3. Add to runner\n",
    "runner = Runner(\n",
    "    models={\n",
    "        \"Naive\": naive_baseline,\n",
    "        \"Zero\": zero_baseline,\n",
    "        \"Random Forest\": rf_model,\n",
    "        #'Ridge': ridge_model,\n",
    "        \"XG Boost\": xgb_model,\n",
    "        \"SVR\": svr_model,\n",
    "    },\n",
    "    cv_splits=4,\n",
    ")\n",
    "\n",
    "# 4. Run all models\n",
    "results = runner.run(df_feat)\n",
    "summary_df = runner.summary()\n",
    "# 5. Show summary\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88981550",
   "metadata": {},
   "source": [
    "## Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812e689",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib\n",
    "\n",
    "\n",
    "def _zscore(series: pd.Series, window: int) -> pd.Series:\n",
    "    return (series - series.rolling(window).mean()) / (\n",
    "        series.rolling(window).std(ddof=0) + 1e-6\n",
    "    )\n",
    "\n",
    "\n",
    "def _safe_div(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    b = b.replace(0, np.nan)\n",
    "    return a / b\n",
    "\n",
    "\n",
    "WINS = [8]\n",
    "\n",
    "FEATURES = [\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"pct_change_price\",\n",
    "    \"percent_change_volume_over_last_wk\",\n",
    "    \"sma_3\",\n",
    "    \"sma_5\",\n",
    "    \"ema_3\",\n",
    "    \"ema_5\",\n",
    "    \"price_ma5_deviation\",\n",
    "    \"rsi_3\",\n",
    "    \"rsi_5\",\n",
    "    \"macd_hist_3_7_3\",\n",
    "    \"momentum_5\",\n",
    "    \"roc_5\",\n",
    "    \"atr_3\",\n",
    "    \"atr_5\",\n",
    "    \"atr_7\",\n",
    "    \"atr_chg_7_3\",\n",
    "    \"volatility_5\",\n",
    "    \"ln_high_low_ratio\",\n",
    "    \"vol_change\",\n",
    "    \"volume_z_5\",\n",
    "    \"price_volume_ratio\",\n",
    "    \"vpt\",\n",
    "    \"close_z_5\",\n",
    "    \"support3w_distance\",\n",
    "    \"cumulative_return_5w\",\n",
    "    \"max_drawdown_5w\",\n",
    "    \"bollinger_band_width_5w\",\n",
    "    \"stochastic_k_5w\",\n",
    "    \"stochastic_d_5w\",\n",
    "]\n",
    "TIME_FEATURES = [\"is_month_end\", \"week_of_month\", \"month\", \"quarter\"]\n",
    "\n",
    "\n",
    "def make_sliding_allow_incomplete(\n",
    "    df_feat: pd.DataFrame, feature_cols, win: int = 8, horizon: int = 1, step: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    samples = []\n",
    "    for stock, group in df_feat.groupby(\"stock\", sort=False):\n",
    "        group = group.reset_index(drop=True)\n",
    "        n = len(group)\n",
    "        max_start = n - horizon\n",
    "        if max_start <= 0:\n",
    "            continue\n",
    "        for start in range(0, max_start, step):\n",
    "            end = min(start + win, n)\n",
    "            window = group.iloc[start:end]\n",
    "            target = group.iloc[min(start + win + horizon - 1, n - 1)]\n",
    "            rec = target[feature_cols].to_dict()\n",
    "            rec.update(\n",
    "                {\n",
    "                    \"win_close_mean\": window[\"close\"].mean(),\n",
    "                    \"win_close_std\": window[\"close\"].std(ddof=0),\n",
    "                    \"win_pctchg_sum\": window[\"pct_change_price\"].sum(),\n",
    "                    \"win_volatility\": window[\"pct_change_price\"].std(ddof=0),\n",
    "                    \"win_rsi_mean\": window[\"rsi_3\"].mean(),\n",
    "                    \"actual_window_len\": len(window),\n",
    "                }\n",
    "            )\n",
    "            rec[\"stock\"] = stock\n",
    "            rec[\"date\"] = target[\"date\"]\n",
    "            rec[\"fall_risk\"] = target[\"fall_risk\"]\n",
    "            samples.append(rec)\n",
    "    return pd.DataFrame(samples)\n",
    "\n",
    "\n",
    "def load_and_preprocess(\n",
    "    path: str | Path, win: int = 8, horizon: int = 1, step: int = 1\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Main Data Preprocessing function\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for col in [\n",
    "        \"open\",\n",
    "        \"high\",\n",
    "        \"low\",\n",
    "        \"close\",\n",
    "        \"volume\",\n",
    "        \"next_weeks_open\",\n",
    "        \"next_weeks_close\",\n",
    "        \"days_to_next_dividend\",\n",
    "        \"percent_return_next_dividend\",\n",
    "    ]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].replace(\"[$,]\", \"\", regex=True).astype(float)\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df = df.sort_values([\"stock\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    enriched = []\n",
    "    for _, group in df.groupby(\"stock\", sort=False):\n",
    "        g = group.copy()\n",
    "        g[\"pct_change_price\"] = g[\"close\"].pct_change()\n",
    "        g[\"percent_change_volume_over_last_wk\"] = g[\"volume\"].pct_change(periods=5)\n",
    "        g[\"sma_3\"] = talib.SMA(g[\"close\"], 3)\n",
    "        g[\"sma_5\"] = talib.SMA(g[\"close\"], 5)\n",
    "        g[\"sma_3_5_diff\"] = g[\"sma_3\"] - g[\"sma_5\"]\n",
    "        g[\"ema_3\"] = talib.EMA(g[\"close\"], 3)\n",
    "        g[\"ema_5\"] = talib.EMA(g[\"close\"], 5)\n",
    "        g[\"ema_diff_3_5\"] = g[\"ema_3\"] - g[\"ema_5\"]\n",
    "        g[\"price_ma5_deviation\"] = (g[\"close\"] / g[\"sma_5\"]) - 1\n",
    "        g[\"rsi_3\"] = talib.RSI(g[\"close\"], 3)\n",
    "        g[\"rsi_5\"] = talib.RSI(g[\"close\"], 5)\n",
    "        g[\"rsi_diff_3_5\"] = g[\"rsi_3\"] - g[\"rsi_5\"]\n",
    "        _, _, macd_hist = talib.MACD(g[\"close\"], 3, 7, 3)\n",
    "        g[\"macd_hist_3_7_3\"] = macd_hist\n",
    "        g[\"momentum_5\"] = g[\"close\"].pct_change(5)\n",
    "        g[\"roc_5\"] = talib.ROC(g[\"close\"], 5)\n",
    "        g[\"atr_3\"] = talib.ATR(g[\"high\"], g[\"low\"], g[\"close\"], 3)\n",
    "        g[\"atr_5\"] = talib.ATR(g[\"high\"], g[\"low\"], g[\"close\"], 5)\n",
    "        g[\"atr_7\"] = talib.ATR(g[\"high\"], g[\"low\"], g[\"close\"], 7)\n",
    "        g[\"atr_chg_7_3\"] = (g[\"atr_7\"] - g[\"atr_3\"]) / g[\"atr_7\"]\n",
    "        g[\"volatility_5\"] = g[\"pct_change_price\"].rolling(5).std()\n",
    "        g[\"high_low_ratio\"] = g[\"high\"] / g[\"low\"]\n",
    "        g[\"ln_high_low_ratio\"] = np.log(g[\"high_low_ratio\"])\n",
    "        g[\"vol_change\"] = g[\"volume\"].pct_change()\n",
    "        g[\"volume_z_5\"] = _zscore(g[\"volume\"], 5)\n",
    "        g[\"price_volume_ratio\"] = _safe_div(\n",
    "            g[\"pct_change_price\"], g[\"percent_change_volume_over_last_wk\"]\n",
    "        )\n",
    "        g[\"vpt\"] = (g[\"pct_change_price\"] * g[\"volume\"]).cumsum()\n",
    "        g[\"close_z_5\"] = _zscore(g[\"close\"], 5)\n",
    "        g[\"support3w_distance\"] = (g[\"close\"] / g[\"low\"].rolling(15).min()) - 1\n",
    "        g[\"is_month_end\"] = g[\"date\"].dt.is_month_end.astype(int)\n",
    "        g[\"week_of_month\"] = (g[\"date\"].dt.day - 1) // 7 + 1\n",
    "        g[\"month\"] = g[\"date\"].dt.month\n",
    "        g[\"quarter\"] = g[\"date\"].dt.quarter\n",
    "        g[\"cumulative_return_5w\"] = (\n",
    "            g[\"close\"].pct_change().add(1).rolling(5).apply(np.prod, raw=True) - 1\n",
    "        )\n",
    "        g[\"max_drawdown_5w\"] = (\n",
    "            g[\"close\"].rolling(5, min_periods=1).max() - g[\"close\"]\n",
    "        ) / g[\"close\"].rolling(5, min_periods=1).max()\n",
    "        rolling_mean = g[\"close\"].rolling(5)\n",
    "        g[\"bollinger_band_width_5w\"] = 4 * rolling_mean.std() / rolling_mean.mean()\n",
    "\n",
    "        lowest_low = g[\"low\"].rolling(5).min()\n",
    "        highest_high = g[\"high\"].rolling(5).max()\n",
    "        g[\"stochastic_k_5w\"] = (\n",
    "            100 * (g[\"close\"] - lowest_low) / (highest_high - lowest_low + 1e-6)\n",
    "        )\n",
    "        g[\"stochastic_d_5w\"] = g[\"stochastic_k_5w\"].rolling(3).mean()\n",
    "\n",
    "        g[FEATURES] = g[FEATURES].ffill()\n",
    "        for c in FEATURES:\n",
    "            g[c] = g[c].fillna(g[c].mean())\n",
    "        g[FEATURES] = g[FEATURES].fillna(0)\n",
    "        enriched.append(g)\n",
    "\n",
    "    df_feat = pd.concat(enriched, ignore_index=True)\n",
    "    df_feat[\"weekly_ret\"] = (df_feat[\"close\"].shift(-1) - df_feat[\"close\"]) / df_feat[\n",
    "        \"close\"\n",
    "    ]\n",
    "    df_feat[\"fall_risk\"] = (df_feat[\"weekly_ret\"] < -0.05).astype(int)\n",
    "\n",
    "    feature_cols = FEATURES + TIME_FEATURES\n",
    "\n",
    "    df_slid = make_sliding_allow_incomplete(\n",
    "        df_feat, feature_cols, win=win, horizon=horizon, step=step\n",
    "    )\n",
    "\n",
    "    return df_slid.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0741bfce",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3827e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.api.losses import MeanSquaredError\n",
    "\n",
    "# Setting the global random seed\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Ensure TensorFlow uses a single thread\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = \"./dow_jones_index.data\"\n",
    "    SAVE_DIR = \"./saved_models/\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    df = load_and_preprocess(DATA_PATH)\n",
    "    feature_cols = df.drop(columns=[\"fall_risk\", \"stock\", \"date\"]).columns\n",
    "\n",
    "    all_X, all_y = [], []\n",
    "    all_idx = []\n",
    "\n",
    "    for stock, group in df.groupby('stock', sort=False):\n",
    "        g = group.sort_values('date')\n",
    "        features = g[feature_cols].values\n",
    "        idx = g.index.values  # raw index\n",
    "\n",
    "        for i in range(len(features) - 8):\n",
    "            all_X.append(features[i:i + 8])\n",
    "            all_y.append(features[i + 8])\n",
    "            all_idx.append(idx[i + 8])\n",
    "\n",
    "    all_X = np.array(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    all_idx = np.array(all_idx)\n",
    "\n",
    "    # split train/test\n",
    "    train_idx, test_idx = [], []\n",
    "    for stock, group in df.groupby('stock', sort=False):\n",
    "        g = group.sort_values('date')\n",
    "        idx = g.index.values\n",
    "        split = len(idx) // 2\n",
    "        train_idx += idx[:split].tolist()\n",
    "        test_idx += idx[split:].tolist()\n",
    "\n",
    "    train_idx = np.array(train_idx)\n",
    "    test_idx = np.array(test_idx)\n",
    "\n",
    "    train_mask = np.isin(all_idx, train_idx)\n",
    "    test_mask = np.isin(all_idx, test_idx)\n",
    "\n",
    "    X_train = all_X[train_mask]\n",
    "    y_train = all_y[train_mask]\n",
    "    X_test = all_X[test_mask]\n",
    "    y_test = all_y[test_mask]\n",
    "\n",
    "    # print(f\"Trainning set sample size: {len(X_train)}, Test set sample size: {len(X_test)}\")\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_tr = scaler.transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "    X_te = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for fold, (train_idx_cv, val_idx_cv) in enumerate(tscv.split(X_tr)):\n",
    "        model = Sequential([\n",
    "            LSTM(64, activation='relu', return_sequences=True, input_shape=(X_tr.shape[1], X_tr.shape[2])),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(X_tr.shape[2])\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss=MeanSquaredError())\n",
    "\n",
    "        model.fit(X_tr[train_idx_cv], y_train[train_idx_cv], validation_data=(X_tr[val_idx_cv], y_train[val_idx_cv]), epochs=30, batch_size=16, verbose=0)\n",
    "\n",
    "        val_loss = model.evaluate(X_tr[val_idx_cv], y_train[val_idx_cv], verbose=0)\n",
    "        # print(f\"Fold {fold+1} validation set Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "    # Use the best model to train again on the full training set\n",
    "    final_model = Sequential([\n",
    "        LSTM(64, activation='relu', return_sequences=True, input_shape=(X_tr.shape[1], X_tr.shape[2])),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(X_tr.shape[2])\n",
    "    ])\n",
    "    final_model.compile(optimizer='adam', loss=MeanSquaredError())\n",
    "    final_model.fit(X_tr, y_train, epochs=30, batch_size=16, verbose=0)\n",
    "\n",
    "    final_model.save(os.path.join(SAVE_DIR, \"LSTM_weekly_forecast.h5\"))\n",
    "    print(\"LSTM Model Save Success!\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss = final_model.evaluate(X_te, y_test, verbose=0)\n",
    "    # print(f\"test set final loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb4c4a",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import GRU, Dense, Dropout\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.api.losses import MeanSquaredError\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = \"./dow_jones_index.data\"\n",
    "    SAVE_DIR = \"./saved_models/\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    df = load_and_preprocess(DATA_PATH)\n",
    "    feature_cols = df.drop(columns=[\"fall_risk\", \"stock\", \"date\"]).columns\n",
    "\n",
    "    all_X, all_y = [], []\n",
    "    all_idx = []\n",
    "\n",
    "    for stock, group in df.groupby(\"stock\", sort=False):\n",
    "        g = group.sort_values(\"date\")\n",
    "        features = g[feature_cols].values\n",
    "        idx = g.index.values\n",
    "\n",
    "        for i in range(len(features) - 8):\n",
    "            all_X.append(features[i : i + 8])\n",
    "            all_y.append(features[i + 8])\n",
    "            all_idx.append(idx[i + 8])\n",
    "\n",
    "    all_X = np.array(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    all_idx = np.array(all_idx)\n",
    "\n",
    "    train_idx, test_idx = [], []\n",
    "    for stock, group in df.groupby(\"stock\", sort=False):\n",
    "        g = group.sort_values(\"date\")\n",
    "        idx = g.index.values\n",
    "        split = len(idx) // 2\n",
    "        train_idx += idx[:split].tolist()\n",
    "        test_idx += idx[split:].tolist()\n",
    "\n",
    "    train_idx = np.array(train_idx)\n",
    "    test_idx = np.array(test_idx)\n",
    "\n",
    "    train_mask = np.isin(all_idx, train_idx)\n",
    "    test_mask = np.isin(all_idx, test_idx)\n",
    "\n",
    "    X_train = all_X[train_mask]\n",
    "    y_train = all_y[train_mask]\n",
    "    X_test = all_X[test_mask]\n",
    "    y_test = all_y[test_mask]\n",
    "\n",
    "    # print(f\"Trainning set sample size: {len(X_train)}, Test set sample size: {len(X_test)}\")\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_tr = scaler.transform(X_train.reshape(X_train.shape[0], -1)).reshape(\n",
    "        X_train.shape\n",
    "    )\n",
    "    X_te = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for fold, (train_idx_cv, val_idx_cv) in enumerate(tscv.split(X_tr)):\n",
    "        model = Sequential(\n",
    "            [\n",
    "                GRU(\n",
    "                    64,\n",
    "                    activation=\"relu\",\n",
    "                    return_sequences=True,\n",
    "                    input_shape=(X_tr.shape[1], X_tr.shape[2]),\n",
    "                ),\n",
    "                Dropout(0.2),\n",
    "                GRU(32, activation=\"relu\"),\n",
    "                Dropout(0.2),\n",
    "                Dense(X_tr.shape[2]),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(optimizer=\"adam\", loss=MeanSquaredError())\n",
    "\n",
    "        model.fit(\n",
    "            X_tr[train_idx_cv],\n",
    "            y_train[train_idx_cv],\n",
    "            validation_data=(X_tr[val_idx_cv], y_train[val_idx_cv]),\n",
    "            epochs=30,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        val_loss = model.evaluate(X_tr[val_idx_cv], y_train[val_idx_cv], verbose=0)\n",
    "        # print(f\"Fold {fold+1} validation set loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "    final_model = Sequential(\n",
    "        [\n",
    "            GRU(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                return_sequences=True,\n",
    "                input_shape=(X_tr.shape[1], X_tr.shape[2]),\n",
    "            ),\n",
    "            Dropout(0.2),\n",
    "            GRU(32, activation=\"relu\"),\n",
    "            Dropout(0.2),\n",
    "            Dense(X_tr.shape[2]),\n",
    "        ]\n",
    "    )\n",
    "    final_model.compile(optimizer=\"adam\", loss=MeanSquaredError())\n",
    "    final_model.fit(X_tr, y_train, epochs=30, batch_size=16, verbose=0)\n",
    "\n",
    "    final_model.save(os.path.join(SAVE_DIR, \"GRU_weekly_forecast.h5\"))\n",
    "    print(\"GRU Model Save Success!\")\n",
    "\n",
    "    test_loss = final_model.evaluate(X_te, y_test, verbose=0)\n",
    "    # print(f\"test set final loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f638bc",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad80ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.api.models import Model\n",
    "from keras.api.layers import (\n",
    "    Input,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    LayerNormalization,\n",
    "    MultiHeadAttention,\n",
    "    Add,\n",
    "    GlobalAveragePooling1D,\n",
    ")\n",
    "from keras.api.losses import MeanSquaredError\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, head_size=64, num_heads=2, ff_dim=128, dropout=0.1):\n",
    "    x = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Add()([x, inputs])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    x_ff = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x_ff = Dense(inputs.shape[-1])(x_ff)\n",
    "    x_ff = Dropout(dropout)(x_ff)\n",
    "    x = Add()([x, x_ff])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = \"./dow_jones_index.data\"\n",
    "    SAVE_DIR = \"./saved_models/\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    df = load_and_preprocess(DATA_PATH)\n",
    "    feature_cols = df.drop(columns=[\"fall_risk\", \"stock\", \"date\"]).columns\n",
    "\n",
    "    all_X, all_y, all_idx = [], [], []\n",
    "\n",
    "    for stock, group in df.groupby(\"stock\", sort=False):\n",
    "        g = group.sort_values(\"date\")\n",
    "        features = g[feature_cols].values\n",
    "        idx = g.index.values\n",
    "\n",
    "        for i in range(len(features) - 8):\n",
    "            all_X.append(features[i : i + 8])\n",
    "            all_y.append(features[i + 8])\n",
    "            all_idx.append(idx[i + 8])\n",
    "\n",
    "    all_X = np.array(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    all_idx = np.array(all_idx)\n",
    "\n",
    "    train_idx, test_idx = [], []\n",
    "    for stock, group in df.groupby(\"stock\", sort=False):\n",
    "        g = group.sort_values(\"date\")\n",
    "        idx = g.index.values\n",
    "        split = len(idx) // 2\n",
    "        train_idx += idx[:split].tolist()\n",
    "        test_idx += idx[split:].tolist()\n",
    "\n",
    "    train_mask = np.isin(all_idx, train_idx)\n",
    "    test_mask = np.isin(all_idx, test_idx)\n",
    "\n",
    "    X_train, y_train = all_X[train_mask], all_y[train_mask]\n",
    "    X_test, y_test = all_X[test_mask], all_y[test_mask]\n",
    "\n",
    "    # print(f\"Trainning set sample size: {len(X_train)}, Test set sample size: {len(X_test)}\")\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_tr = scaler.transform(X_train.reshape(X_train.shape[0], -1)).reshape(\n",
    "        X_train.shape\n",
    "    )\n",
    "    X_te = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "\n",
    "    # Cross-validation to select the best model structure\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for fold, (train_idx_cv, val_idx_cv) in enumerate(tscv.split(X_tr)):\n",
    "        input_layer = Input(shape=(X_tr.shape[1], X_tr.shape[2]))\n",
    "        x = transformer_encoder(input_layer, head_size=64, num_heads=2, ff_dim=128)\n",
    "        x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        output_layer = Dense(X_tr.shape[2])(x)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(optimizer=\"adam\", loss=MeanSquaredError())\n",
    "\n",
    "        model.fit(\n",
    "            X_tr[train_idx_cv],\n",
    "            y_train[train_idx_cv],\n",
    "            validation_data=(X_tr[val_idx_cv], y_train[val_idx_cv]),\n",
    "            epochs=30,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        val_loss = model.evaluate(X_tr[val_idx_cv], y_train[val_idx_cv], verbose=0)\n",
    "        # print(f\"Fold {fold+1} validation set loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "    input_layer = Input(shape=(X_tr.shape[1], X_tr.shape[2]))\n",
    "    x = transformer_encoder(input_layer, head_size=64, num_heads=2, ff_dim=128)\n",
    "    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    output_layer = Dense(X_tr.shape[2])(x)\n",
    "\n",
    "    final_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    final_model.compile(optimizer=\"adam\", loss=MeanSquaredError())\n",
    "    final_model.fit(X_tr, y_train, epochs=30, batch_size=16, verbose=0)\n",
    "\n",
    "    final_model.save(os.path.join(SAVE_DIR, \"transformer_weekly_forecast.h5\"))\n",
    "    print(\"Transformer Model Save Success!\")\n",
    "\n",
    "    test_loss = final_model.evaluate(X_te, y_test, verbose=0)\n",
    "    # print(f\"test set final Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e0094",
   "metadata": {},
   "source": [
    "### LSTM-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a47dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from keras.api.models import Model\n",
    "from keras.api.layers import Input, LSTM, Dense, Dropout, Layer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.api.losses import MeanSquaredError\n",
    "from keras.api.optimizers import Adam\n",
    "import tensorflow.python.keras.backend as K\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "# Customized Attention Mechanisms\n",
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, return_sequences=True, **kwargs):\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.V = self.add_weight(\n",
    "            name=\"att_var\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = K.tanh(K.dot(inputs, self.W) + self.b)\n",
    "        attention_weights = K.softmax(K.dot(score, self.V), axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = K.sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "# Create Model\n",
    "def build_lstm_attention_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(64, return_sequences=True)(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = SelfAttention()(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    outputs = Dense(input_shape[1])(x)  # feature dimension\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(), loss=MeanSquaredError())\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"./saved_models\", exist_ok=True)\n",
    "    DATA_PATH = \"./dow_jones_index.data\"\n",
    "    df = load_and_preprocess(DATA_PATH)\n",
    "    feature_cols = df.drop(columns=[\"fall_risk\", \"stock\", \"date\"]).columns\n",
    "\n",
    "    all_X, all_y, all_idx = [], [], []\n",
    "    for stock, group in df.groupby(\"stock\", sort=False):\n",
    "        g = group.sort_values(\"date\")\n",
    "        features = g[feature_cols].values\n",
    "        idx = g.index.values\n",
    "        for i in range(len(features) - 8):\n",
    "            all_X.append(features[i : i + 8])\n",
    "            all_y.append(features[i + 8])\n",
    "            all_idx.append(idx[i + 8])\n",
    "\n",
    "    all_X = np.array(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    all_idx = np.array(all_idx)\n",
    "\n",
    "    train_idx, test_idx = [], []\n",
    "    for stock, group in df.groupby(\"stock\", sort=False):\n",
    "        g = group.sort_values(\"date\")\n",
    "        idx = g.index.values\n",
    "        split = len(idx) // 2\n",
    "        train_idx += idx[:split].tolist()\n",
    "        test_idx += idx[split:].tolist()\n",
    "\n",
    "    train_mask = np.isin(all_idx, train_idx)\n",
    "    test_mask = np.isin(all_idx, test_idx)\n",
    "    X_train = all_X[train_mask]\n",
    "    y_train = all_y[train_mask]\n",
    "    X_test = all_X[test_mask]\n",
    "    y_test = all_y[test_mask]\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_tr = scaler.transform(X_train.reshape(X_train.shape[0], -1)).reshape(\n",
    "        X_train.shape\n",
    "    )\n",
    "    X_te = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for fold, (train_idx_cv, val_idx_cv) in enumerate(tscv.split(X_tr)):\n",
    "        model = build_lstm_attention_model(input_shape=(X_tr.shape[1], X_tr.shape[2]))\n",
    "        model.fit(\n",
    "            X_tr[train_idx_cv],\n",
    "            y_train[train_idx_cv],\n",
    "            validation_data=(X_tr[val_idx_cv], y_train[val_idx_cv]),\n",
    "            epochs=30,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "        val_loss = model.evaluate(X_tr[val_idx_cv], y_train[val_idx_cv], verbose=0)\n",
    "        # print(f\"Fold {fold+1} validation set loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "\n",
    "    final_model = build_lstm_attention_model(input_shape=(X_tr.shape[1], X_tr.shape[2]))\n",
    "    final_model.fit(X_tr, y_train, epochs=30, batch_size=16, verbose=0)\n",
    "    final_model.save(\"./saved_models/lstm_attn_weekly_forecast.h5\")\n",
    "    print(\"LSTM + Attention Model Save Success!\")\n",
    "\n",
    "    test_loss = final_model.evaluate(X_te, y_test, verbose=0)\n",
    "    # print(f\"test set final loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.api.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "model_list = [\"lstm\", \"gru\", \"transformer\", \"lstm_attn\"]\n",
    "\n",
    "\n",
    "def create_latest_input(X_all, input_weeks=8):\n",
    "    return np.expand_dims(X_all[-input_weeks:], axis=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = \"./dow_jones_index.data\"\n",
    "    df = load_and_preprocess(DATA_PATH, win=8, horizon=1)\n",
    "\n",
    "    feature_cols = df.drop(columns=[\"fall_risk\", \"stock\", \"date\"]).columns\n",
    "    X_all = df[feature_cols].values\n",
    "\n",
    "    scaler = StandardScaler().fit(X_all)\n",
    "    X_all = scaler.transform(X_all)\n",
    "\n",
    "    for model_name in model_list:\n",
    "        MODEL_PATH = f\"./saved_models/{model_name}_weekly_forecast.h5\"\n",
    "        CSV_PATH = f\"./saved_models/{model_name}_predicted_next_week.csv\"\n",
    "\n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            print(f\"Model files not exist: {MODEL_PATH}\")\n",
    "            continue\n",
    "\n",
    "        # Using custom_objects for lstm_attn model\n",
    "        if model_name == \"lstm_attn\":\n",
    "            try:\n",
    "                model = load_model(\n",
    "                    MODEL_PATH, custom_objects={\"SelfAttention\": SelfAttention}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load lstm_attn model: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            model = load_model(MODEL_PATH)\n",
    "\n",
    "        print(f\"Model loaded successfully: {model_name}\")\n",
    "\n",
    "        X_input = create_latest_input(X_all, input_weeks=8)\n",
    "        y_pred = model.predict(X_input)\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "\n",
    "        pred_df = pd.DataFrame([y_pred], columns=feature_cols)\n",
    "        pred_df.to_csv(CSV_PATH, index=False)\n",
    "        print(f\"Saving the forecast features for the next week to {CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def tune_threshold(y_true, proba):\n",
    "    best_thr, best_f1 = 0.5, 0.0\n",
    "    for thr in np.linspace(0.001, 0.999, 999):\n",
    "        preds = (proba >= thr).astype(int)\n",
    "        f1 = f1_score(y_true, preds, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_thr, best_f1 = thr, f1\n",
    "    return best_thr\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = \"./dow_jones_index.data\"\n",
    "    SAVE_DIR = \"./saved_models/\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    # Data loading and preprocessing\n",
    "    df = load_and_preprocess(DATA_PATH)\n",
    "    # print(f\"Sample size after sliding: {len(df)}, Number of positive examples: {df['fall_risk'].sum()}\")\n",
    "\n",
    "    features = df.drop(columns=[\"fall_risk\", \"stock\", \"date\"]).columns\n",
    "    X = df[features].values\n",
    "    y = df[\"fall_risk\"].values\n",
    "\n",
    "    # split train/test\n",
    "    train_idx, test_idx = [], []\n",
    "    for stock, g in df.groupby(\"stock\", sort=False):\n",
    "        idx = g.sort_values(\"date\").index\n",
    "        split = len(idx) // 2\n",
    "        train_idx += idx[:split].tolist()\n",
    "        test_idx += idx[split:].tolist()\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    # Standardization & Missing value filling\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_tr = scaler.transform(X_train)\n",
    "    X_te = scaler.transform(X_test)\n",
    "    imputer = SimpleImputer(strategy=\"mean\").fit(X_tr)\n",
    "    X_tr = imputer.transform(X_tr)\n",
    "    X_te = imputer.transform(X_te)\n",
    "\n",
    "    # SMOTE\n",
    "    minority = np.sum(y_train == 1)\n",
    "    if minority >= 2:\n",
    "        k = min(5, minority - 1)\n",
    "        X_tr, y_tr = SMOTE(random_state=42, k_neighbors=k).fit_resample(X_tr, y_train)\n",
    "    else:\n",
    "        y_tr = y_train\n",
    "    # print(f\"Number of samples in the training set after oversampling: {len(X_tr)}\")\n",
    "\n",
    "    # Time Series Cross-Validation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # Grid Search for HyperParameters\n",
    "    grid_xgb = GridSearchCV(\n",
    "        XGBClassifier(eval_metric=\"logloss\", random_state=42),\n",
    "        {\"max_depth\": [3, 5], \"learning_rate\": [0.05, 0.1], \"n_estimators\": [100, 150]},\n",
    "        scoring=\"f1\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_rf = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\"n_estimators\": [100, 150], \"max_depth\": [5, 8]},\n",
    "        scoring=\"f1\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_mlp = GridSearchCV(\n",
    "        MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42),\n",
    "        {\"alpha\": [0.001]},\n",
    "        scoring=\"f1\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_lr = GridSearchCV(\n",
    "        LogisticRegression(max_iter=500, class_weight=\"balanced\", random_state=42),\n",
    "        {\"C\": [1.0, 10.0]},\n",
    "        scoring=\"f1\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_svm = GridSearchCV(\n",
    "        SVC(probability=True, random_state=42),\n",
    "        {\"C\": [1.0, 10.0], \"kernel\": [\"rbf\", \"linear\"]},\n",
    "        scoring=\"f1\",\n",
    "        cv=tscv,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Train models\n",
    "    grid_xgb.fit(X_tr, y_tr)\n",
    "    grid_rf.fit(X_tr, y_tr)\n",
    "    grid_mlp.fit(X_tr, y_tr)\n",
    "    grid_lr.fit(X_tr, y_tr)\n",
    "    grid_svm.fit(X_tr, y_tr)\n",
    "\n",
    "    best_xgb = grid_xgb.best_estimator_\n",
    "    best_rf = grid_rf.best_estimator_\n",
    "    best_mlp = grid_mlp.best_estimator_\n",
    "    best_lr = grid_lr.best_estimator_\n",
    "    best_svm = grid_svm.best_estimator_\n",
    "    model_scores = {\n",
    "        \"xgb\": grid_xgb.best_score_,\n",
    "        \"rf\": grid_rf.best_score_,\n",
    "        \"mlp\": grid_mlp.best_score_,\n",
    "        \"lr\": grid_lr.best_score_,\n",
    "    }\n",
    "\n",
    "    max_score = max(model_scores.values())\n",
    "    weights = [score / max_score for score in model_scores.values()]\n",
    "\n",
    "    print(\"\\nOptimal Hyperparameters:\")\n",
    "    print(\"XGB:\", grid_xgb.best_params_)\n",
    "    print(\"RF :\", grid_rf.best_params_)\n",
    "    print(\"MLP:\", grid_mlp.best_params_)\n",
    "    print(\"LogReg:\", grid_lr.best_params_)\n",
    "\n",
    "    # Voting\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[\n",
    "            (\"xgb\", best_xgb),\n",
    "            (\"rf\", best_rf),\n",
    "            (\"mlp\", best_mlp),\n",
    "            (\"lr\", best_lr),\n",
    "        ],\n",
    "        voting=\"soft\",\n",
    "        weights=weights,\n",
    "    )\n",
    "\n",
    "    # Train Voting & find global best threshold\n",
    "    voting.fit(X_tr, y_tr)\n",
    "    proba = voting.predict_proba(X_te)[:, 1]\n",
    "    global_best_thr = tune_threshold(y_test, proba)\n",
    "\n",
    "    preds = (proba >= global_best_thr).astype(int)\n",
    "    roc = roc_auc_score(y_test, proba)\n",
    "    pr = average_precision_score(y_test, proba)\n",
    "    f1 = f1_score(y_test, preds, zero_division=0)\n",
    "\n",
    "    print(\"Overall test set evaluation (global optimal thresholds):\")\n",
    "    print(\n",
    "        f\"ROC-AUC = {roc:.4f} | PR-AUC = {pr:.4f} | F1 = {f1:.4f} | best_thr = {global_best_thr:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Save Models\n",
    "    SAVE_PATH = os.path.join(SAVE_DIR, \"final_voting_model.pkl\")\n",
    "    with open(SAVE_PATH, \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                \"model\": voting,\n",
    "                \"scaler\": scaler,\n",
    "                \"imputer\": imputer,\n",
    "                \"best_threshold\": global_best_thr,\n",
    "            },\n",
    "            f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "import os\n",
    "from keras.api.models import load_model\n",
    "\n",
    "model_list = [\"lstm\", \"gru\", \"transformer\", \"lstm_attn\"]\n",
    "SAVE_DIR = \"./saved_models/\"\n",
    "VOTING_PATH = os.path.join(SAVE_DIR, \"final_voting_model.pkl\")\n",
    "DATA_PATH = \"./dow_jones_index.data\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_and_preprocess(DATA_PATH, win=8, horizon=1)\n",
    "    feature_cols = df.drop(columns=[\"fall_risk\", \"stock\", \"date\"]).columns\n",
    "    X = df[feature_cols].values\n",
    "    y = df[\"fall_risk\"].values\n",
    "\n",
    "    test_idx = []\n",
    "    for stock, group in df.groupby(\"stock\", sort=False):\n",
    "        idx = group.sort_values(\"date\").index\n",
    "        split = len(idx) // 2\n",
    "        test_idx += idx[split:].tolist()\n",
    "    test_mask = np.isin(df.index.values, test_idx)\n",
    "    X_test = X[test_mask]\n",
    "    y_test = y[test_mask]\n",
    "\n",
    "    with open(VOTING_PATH, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    voting_model = obj[\"model\"]\n",
    "    scaler = obj[\"scaler\"]\n",
    "    best_thr = obj[\"best_threshold\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name in model_list:\n",
    "        MODEL_PATH = f\"{SAVE_DIR}/{model_name}_weekly_forecast.h5\"\n",
    "\n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            print(f\"Model files not exist: {MODEL_PATH}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if model_name == \"lstm_attn\":\n",
    "                model = load_model(\n",
    "                    MODEL_PATH, custom_objects={\"SelfAttention\": SelfAttention}\n",
    "                )\n",
    "            else:\n",
    "                model = load_model(MODEL_PATH)\n",
    "            print(f\"Loading Model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model ({model_name}): {e}\")\n",
    "            continue\n",
    "\n",
    "        n_sample = X_test.shape[0] - 8\n",
    "        if n_sample <= 0:\n",
    "            print(f\"Test set too small to construct input sequence!\")\n",
    "            continue\n",
    "\n",
    "        seq_X = np.array([X_test[i : i + 8] for i in range(n_sample)])\n",
    "        input_scaler = StandardScaler().fit(seq_X.reshape(seq_X.shape[0], -1))\n",
    "        seq_X_scaled = input_scaler.transform(\n",
    "            seq_X.reshape(seq_X.shape[0], -1)\n",
    "        ).reshape(seq_X.shape)\n",
    "\n",
    "        y_pred_feat = model.predict(seq_X_scaled)\n",
    "        X_pred_scaled = scaler.transform(y_pred_feat)\n",
    "        y_proba = voting_model.predict_proba(X_pred_scaled)[:, 1]\n",
    "        y_pred = (y_proba >= best_thr).astype(int)\n",
    "\n",
    "        aligned_y_true = y_test[8 : 8 + len(y_pred)]\n",
    "\n",
    "        f1 = f1_score(aligned_y_true, y_pred)\n",
    "        roc = roc_auc_score(aligned_y_true, y_proba)\n",
    "        pr = average_precision_score(aligned_y_true, y_proba)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"F1-score\": f\"{f1:.4f}\",\n",
    "                \"ROC-AUC\": f\"{roc:.4f}\",\n",
    "                \"PR-AUC\": f\"{pr:.4f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\\nPredictive scores of the four models on the test set:\")\n",
    "    print(pd.DataFrame(results).to_string(index=False))\n",
    "    \n",
    "# Draw model performance visualization comparisons\n",
    "# Convert the result to a DataFrame and make sure the values are float\n",
    "df = pd.DataFrame(results)\n",
    "df[\"Model\"] = (\n",
    "    df[\"Model\"]\n",
    "    .str.upper()\n",
    "    .str.replace(\"_\", \" + \")\n",
    "    .str.replace(\"LSTM + ATTN\", \"LSTM + Attention\")\n",
    ")\n",
    "df[\"F1-score\"] = df[\"F1-score\"].astype(float)\n",
    "df[\"ROC-AUC\"] = df[\"ROC-AUC\"].astype(float)\n",
    "df[\"PR-AUC\"] = df[\"PR-AUC\"].astype(float)\n",
    "\n",
    "x = range(len(df))\n",
    "bar_width = 0.25\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([i - bar_width for i in x], df[\"F1-score\"], width=bar_width, label=\"F1-score\")\n",
    "plt.bar(x, df[\"ROC-AUC\"], width=bar_width, label=\"ROC-AUC\")\n",
    "plt.bar([i + bar_width for i in x], df[\"PR-AUC\"], width=bar_width, label=\"PR-AUC\")\n",
    "\n",
    "plt.xticks(x, df[\"Model\"])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Performance Comparison of Forecasting Models (Voting Classifier)\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44abb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "model_list = [\"lstm\", \"gru\", \"transformer\", \"lstm_attn\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SAVE_DIR = \"./saved_models/\"\n",
    "    MODEL_PATH = os.path.join(SAVE_DIR, \"final_voting_model.pkl\")\n",
    "\n",
    "    with open(MODEL_PATH, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    voting_model = obj[\"model\"]\n",
    "    scaler = obj[\"scaler\"]\n",
    "    best_thr = obj[\"best_threshold\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model_name in model_list:\n",
    "        CSV_PATH = f\"{SAVE_DIR}/{model_name}_predicted_next_week.csv\"\n",
    "\n",
    "        if not os.path.exists(CSV_PATH):\n",
    "            print(f\"Predictive feature file not exist: {CSV_PATH}\")\n",
    "            continue\n",
    "\n",
    "        pred_df = pd.read_csv(CSV_PATH)\n",
    "        X = scaler.transform(pred_df.values)\n",
    "\n",
    "        proba = voting_model.predict_proba(X)[:, 1]\n",
    "        label = (proba >= best_thr).astype(int)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Fall Risk Proba\": f\"{proba[0]:.4f}\",\n",
    "                \"Fall Risk Label\": int(label[0]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Print Comparison table\n",
    "    print(\"\\nComparison of fall risk predictions of different models:\")\n",
    "    print(pd.DataFrame(results).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
